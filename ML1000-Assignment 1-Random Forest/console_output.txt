> randomForest_model

Call:
 randomForest(formula = did_crash_happen ~ ., data = trainData_downsampled,      importance = TRUE, do.trace = 10) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 34.76%
Confusion matrix:
        no    yes class.error
no  161022  95582   0.3724883
yes  82794 173810   0.3226528
> summary(randomForest_model)
                Length  Class  Mode     
call                  5 -none- call     
type                  1 -none- character
predicted        513208 factor numeric  
err.rate           1500 -none- numeric  
confusion             6 -none- numeric  
votes           1026416 matrix numeric  
oob.times        513208 -none- numeric  
classes               2 -none- character
importance           24 -none- numeric  
importanceSD         18 -none- numeric  
localImportance       0 -none- NULL     
proximity             0 -none- NULL     
ntree                 1 -none- numeric  
mtry                  1 -none- numeric  
forest               14 -none- list     
y                513208 factor numeric  
test                  0 -none- NULL     
inbag                 0 -none- NULL     
terms                 3 terms  call     
> predict(randomForest_model, newdata = testData, type="prob")
           no   yes
48894   0.912 0.088
48896   0.798 0.202
48906   0.506 0.494
48917   0.864 0.136
48919   0.828 0.172
48922   0.506 0.494
48925   0.416 0.584
48929   0.376 0.624
48938   0.980 0.020
48946   0.200 0.800
48950   0.272 0.728
48952   0.254 0.746
48961   0.858 0.142
48962   0.952 0.048
48981   0.688 0.312
48983   0.640 0.360
48985   0.614 0.386
48991   0.814 0.186
48994   0.602 0.398
49001   0.602 0.398
49002   0.514 0.486
49008   0.436 0.564
49011   0.892 0.108
49015   0.698 0.302
49026   0.554 0.446
49031   0.726 0.274
49036   0.784 0.216
49037   0.864 0.136
49038   0.866 0.134
49041   0.908 0.092
49048   0.656 0.344
49052   0.732 0.268
49054   0.646 0.354
49055   0.880 0.120
49057   0.332 0.668
49072   0.334 0.666
49077   0.400 0.600
49088   0.534 0.466
49092   0.158 0.842
49097   0.302 0.698
49103   0.582 0.418
49110   0.890 0.110
49111   0.586 0.414
49129   0.580 0.420
49134   0.836 0.164
49135   0.676 0.324
49143   0.202 0.798
49144   0.196 0.804
49147   0.262 0.738
49148   0.654 0.346
49155   0.742 0.258
49160   0.628 0.372
49163   0.300 0.700
49182   0.894 0.106
49191   0.398 0.602
49195   0.162 0.838
49207   0.626 0.374
49208   0.886 0.114
49209   0.748 0.252
49214   0.566 0.434
49216   0.224 0.776
49219   0.216 0.784
49226   0.238 0.762
49229   0.416 0.584
49237   0.716 0.284
49240   0.500 0.500
49245   0.754 0.246
49249   0.772 0.228
49253   0.932 0.068
49254   0.846 0.154
49267   0.512 0.488
49268   0.638 0.362
49277   0.896 0.104
49280   0.422 0.578
49284   0.306 0.694
49287   0.236 0.764
49289   0.266 0.734
49290   0.426 0.574
49291   0.362 0.638
49295   0.748 0.252
49302   0.740 0.260
49305   0.348 0.652
49306   0.330 0.670
49307   0.352 0.648
49310   0.270 0.730
49311   0.426 0.574
49313   0.400 0.600
49315   0.534 0.466
49322   0.698 0.302
49324   0.634 0.366
49329   0.408 0.592
49330   0.442 0.558
49333   0.312 0.688
49334   0.398 0.602
49338   0.252 0.748
49345   0.492 0.508
49350   0.810 0.190
49353   0.482 0.518
49355   0.440 0.560
49360   0.226 0.774
49363   0.206 0.794
49367   0.594 0.406
49369   0.456 0.544
49372   0.532 0.468
49383   0.248 0.752
49392   0.596 0.404
49406   0.620 0.380
49408   0.356 0.644
49426   0.536 0.464
49427   0.584 0.416
49429   0.612 0.388
49430   0.584 0.416
49434   0.558 0.442
49436   0.652 0.348
49441   0.786 0.214
49444   0.952 0.048
49449   0.342 0.658
49452   0.416 0.584
49462   0.486 0.514
49468   0.928 0.072
49471   0.714 0.286
49473   0.148 0.852
49475   0.148 0.852
49476   0.188 0.812
49478   0.126 0.874
49479   0.108 0.892
49480   0.234 0.766
49482   0.166 0.834
49483   0.178 0.822
49488   0.784 0.216
49490   0.726 0.274
49491   0.582 0.418
49504   0.248 0.752
49507   0.080 0.920
49516   0.678 0.322
49519   0.712 0.288
49520   0.752 0.248
49529   0.342 0.658
49531   0.254 0.746
49532   0.354 0.646
49536   0.432 0.568
49540   0.600 0.400
49556   0.136 0.864
49562   0.480 0.520
49565   0.440 0.560
49567   0.504 0.496
49581   0.664 0.336
49584   0.876 0.124
49592   0.692 0.308
49599   0.120 0.880
49601   0.160 0.840
49606   0.924 0.076
49608   0.868 0.132
49609   0.810 0.190
49626   0.428 0.572
49632   0.862 0.138
49633   0.688 0.312
49640   0.556 0.444
49644   0.484 0.516
49659   0.884 0.116
49662   0.910 0.090
49663   0.612 0.388
49664   0.456 0.544
49665   0.354 0.646
49667   0.122 0.878
49669   0.190 0.810
49670   0.248 0.752
49671   0.210 0.790
49672   0.258 0.742
49674   0.256 0.744
49684   0.828 0.172
49691   0.350 0.650
49693   0.448 0.552
49696   0.210 0.790
49697   0.136 0.864
49703   0.542 0.458
49706   0.538 0.462
49712   0.904 0.096
49715   0.628 0.372
49723   0.484 0.516
49724   0.458 0.542
49725   0.584 0.416
49731   0.382 0.618
49742   0.628 0.372
49745   0.628 0.372
49748   0.458 0.542
49749   0.550 0.450
49773   0.864 0.136
49787   0.242 0.758
49790   0.218 0.782
49797   0.604 0.396
49798   0.860 0.140
49803   0.818 0.182
49810   0.354 0.646
49812   0.274 0.726
49819   0.398 0.602
49824   0.750 0.250
49830   0.924 0.076
49854   0.914 0.086
49855   0.812 0.188
49858   0.358 0.642
49866   0.140 0.860
49868   0.138 0.862
49871   0.220 0.780
49872   0.424 0.576
49874   0.812 0.188
49880   0.794 0.206
49883   0.604 0.396
49891   0.374 0.626
49898   0.664 0.336
49911   0.616 0.384
49917   0.694 0.306
49920   0.890 0.110
49932   0.202 0.798
49934   0.166 0.834
49937   0.328 0.672
49940   0.736 0.264
49951   0.900 0.100
49952   0.636 0.364
49953   0.144 0.856
49961   0.096 0.904
49964   0.264 0.736
49976   0.546 0.454
49984   0.144 0.856
49997   0.918 0.082
50003   0.246 0.754
50005   0.360 0.640
50021   0.830 0.170
50026   0.242 0.758
50028   0.120 0.880
50039   0.400 0.600
50054   0.548 0.452
50055   0.468 0.532
50065   0.630 0.370
50068   0.396 0.604
50069   0.848 0.152
50081   0.504 0.496
50086   0.358 0.642
50090   0.762 0.238
50091   0.966 0.034
50098   0.230 0.770
50099   0.168 0.832
50101   0.108 0.892
50102   0.152 0.848
50114   0.890 0.110
50117   0.954 0.046
50120   0.694 0.306
50129   0.110 0.890
50130   0.208 0.792
50137   0.504 0.496
50149   0.404 0.596
50150   0.458 0.542
50153   0.568 0.432
50155   0.652 0.348
50158   0.622 0.378
50160   0.518 0.482
50166   0.808 0.192
50169   0.214 0.786
50174   0.460 0.540
50175   0.540 0.460
50178   0.516 0.484
50182   0.138 0.862
50187   0.636 0.364
50190   0.560 0.440
50214   0.744 0.256
50218   0.378 0.622
50222   0.394 0.606
50228   0.280 0.720
50232   0.464 0.536
50241   0.710 0.290
50251   0.576 0.424
50261   0.900 0.100
50265   0.368 0.632
50266   0.236 0.764
50270   0.140 0.860
50281   0.690 0.310
50285   0.930 0.070
50300   0.430 0.570
50303   0.740 0.260
50311   0.718 0.282
50321   0.114 0.886
50328   0.710 0.290
50334   0.792 0.208
50335   0.754 0.246
50337   0.212 0.788
50347   0.152 0.848
50356   0.802 0.198
50357   0.604 0.396
50363   0.604 0.396
50369   0.070 0.930
50380   0.764 0.236
50386   0.384 0.616
50388   0.158 0.842
50390   0.212 0.788
50392   0.272 0.728
50394   0.320 0.680
50398   0.402 0.598
50400   0.372 0.628
50402   0.792 0.208
50403   0.878 0.122
50406   0.802 0.198
50412   0.448 0.552
50419   0.294 0.706
50423   0.694 0.306
50424   0.708 0.292
50425   0.856 0.144
50427   0.978 0.022
50434   0.280 0.720
50435   0.380 0.620
50436   0.390 0.610
50437   0.348 0.652
50438   0.256 0.744
50441   0.218 0.782
50447   0.682 0.318
50465   0.116 0.884
50468   0.506 0.494
50469   0.466 0.534
50473   0.652 0.348
50477   0.974 0.026
50478   0.856 0.144
50487   0.272 0.728
50497   0.796 0.204
50509   0.372 0.628
50523   0.924 0.076
50527   0.706 0.294
50542   0.412 0.588
50546   0.692 0.308
50550   0.742 0.258
50552   0.724 0.276
50556   0.668 0.332
50559   0.726 0.274
50560   0.594 0.406
50570   0.762 0.238
50573   0.432 0.568
50577   0.858 0.142
50581   0.280 0.720
50585   0.388 0.612
50610   0.318 0.682
50619   0.962 0.038
50621   0.944 0.056
50625   0.276 0.724
50628   0.392 0.608
50631   0.122 0.878
50632   0.492 0.508
50634   0.410 0.590
50653   0.080 0.920
50673   0.202 0.798
50680   0.102 0.898
50682   0.146 0.854
50683   0.184 0.816
50696   0.626 0.374
50701   0.164 0.836
50702   0.096 0.904
50707   0.152 0.848
50718   0.702 0.298
50727   0.306 0.694
50738   0.564 0.436
50745   0.742 0.258
50747   0.638 0.362
50753   0.448 0.552
50754   0.432 0.568
50756   0.392 0.608
50763   0.884 0.116
50765   0.952 0.048
50774   0.358 0.642
50775   0.326 0.674
50788   0.978 0.022
50794   0.502 0.498
50799   0.428 0.572
50800   0.700 0.300
50801   0.458 0.542
50805   0.590 0.410
50820   0.258 0.742
50824   0.484 0.516
50831   0.458 0.542
50842   0.222 0.778
50843   0.308 0.692
50845   0.122 0.878
50853   0.652 0.348
50858   0.380 0.620
50868   0.346 0.654
50869   0.162 0.838
50871   0.108 0.892
50873   0.360 0.640
50879   0.302 0.698
50888   0.680 0.320
50892   0.314 0.686
50894   0.404 0.596
50900   0.334 0.666
50901   0.432 0.568
50907   0.468 0.532
50922   0.482 0.518
50923   0.436 0.564
50925   0.412 0.588
50926   0.580 0.420
50930   0.774 0.226
50931   0.718 0.282
50933   0.684 0.316
50934   0.758 0.242
50940   0.460 0.540
50943   0.160 0.840
50950   0.696 0.304
50953   0.782 0.218
50956   0.908 0.092
50958   0.838 0.162
50960   0.660 0.340
50963   0.274 0.726
50964   0.518 0.482
50967   0.196 0.804
50974   0.644 0.356
50977   0.420 0.580
50978   0.842 0.158
50999   0.696 0.304
51000   0.756 0.244
51001   0.750 0.250
51007   0.860 0.140
51010   0.348 0.652
51011   0.218 0.782
51021   0.626 0.374
51033   0.378 0.622
51036   0.248 0.752
51040   0.056 0.944
51048   0.278 0.722
51050   0.718 0.282
51054   0.790 0.210
51055   0.710 0.290
51056   0.658 0.342
51059   0.264 0.736
51064   0.260 0.740
51068   0.298 0.702
51076   0.476 0.524
51080   0.526 0.474
51082   0.632 0.368
51084   0.438 0.562
51085   0.510 0.490
51086   0.660 0.340
51087   0.718 0.282
51107   0.396 0.604
51112   0.676 0.324
51113   0.570 0.430
51114   0.584 0.416
51115   0.568 0.432
51120   0.550 0.450
51125   0.854 0.146
51128   0.266 0.734
51129   0.400 0.600
51146   0.918 0.082
51151   0.676 0.324
51155   0.082 0.918
51158   0.132 0.868
51160   0.080 0.920
51163   0.346 0.654
51164   0.586 0.414
51165   0.328 0.672
51167   0.402 0.598
51177   0.338 0.662
51178   0.350 0.650
51188   0.574 0.426
51190   0.284 0.716
51194   0.846 0.154
51198   0.916 0.084
51201   0.428 0.572
51202   0.472 0.528
51205   0.578 0.422
51207   0.550 0.450
51219   0.698 0.302
51220   0.898 0.102
51221   0.838 0.162
51222   0.860 0.140
51224   0.850 0.150
51225   0.856 0.144
51226   0.816 0.184
51231   0.436 0.564
51236   0.446 0.554
51244   0.402 0.598
51253   0.432 0.568
51260   0.360 0.640
51266   0.768 0.232
51277   0.404 0.596
51278   0.432 0.568
51283   0.166 0.834
51285   0.726 0.274
51290   0.844 0.156
51292   0.946 0.054
51301   0.308 0.692
51308   0.504 0.496
51315   0.892 0.108
51316   0.840 0.160
51318   0.742 0.258
51319   0.564 0.436
51321   0.284 0.716
51339   0.820 0.180
51341   0.798 0.202
51347   0.278 0.722
51361   0.262 0.738
51364   0.868 0.132
51373   0.074 0.926
51382   0.226 0.774
51386   0.642 0.358
51393   0.840 0.160
 [ reached getOption("max.print") -- omitted 268963 rows ]
attr(,"class")
[1] "matrix" "votes" 
> model_rf = randomForest_model
> rm(randomForest_model)
> pred.model_rf.prob = predict(randomForest_model, newdata = testData, type="prob")
Error in predict(randomForest_model, newdata = testData, type = "prob") : 
  object 'randomForest_model' not found
> pred.model_rf.prob = predict(model_rf, newdata = testData, type="prob")
> pred.model_rf.raw = predict(model_rf, newdata = testData)
> roc.model_rf = pROC::roc(
+   testData$did_crash_happen, 
+   as.vector(ifelse(pred.model_rf.prob[,"yes"] >0.5, 1,0))
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_rf = pROC::auc(roc.model_rf)
> print(auc.model_rf)
Area under the curve: 0.6533
> plot.roc(roc.model_rf, print.auc = TRUE, col = 'red' , print.thres = "best" )
> confusionMatrix(data = pred.model_rf.raw, testData$did_crash_happen)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  128801  20698
       yes  76273  43691
                                          
               Accuracy : 0.6401          
                 95% CI : (0.6383, 0.6419)
    No Information Rate : 0.761           
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.2366          
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.6281          
            Specificity : 0.6785          
         Pos Pred Value : 0.8616          
         Neg Pred Value : 0.3642          
             Prevalence : 0.7610          
         Detection Rate : 0.4780          
   Detection Prevalence : 0.5548          
      Balanced Accuracy : 0.6533          
                                          
       'Positive' Class : no              
                                  