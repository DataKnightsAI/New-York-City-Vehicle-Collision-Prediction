> model_knn <- knn(
+   trainData_downsampled_without_response_var,
+   testData_without_response_var,
+   cl = trainData_downsampled_response_column,
+   k=9,
+   prob = TRUE #create probabilities so we can plot ROC
+ )
> proc.time() - ptm_rf
   user  system elapsed 
 636.00    0.16  639.03 
> 
> #probabilities 
> attributes(model_knn)$prob
   [1] 0.7777778 0.8000000 0.5555556 0.8888889 1.0000000 0.5555556 0.7777778 0.5555556 0.8888889 0.6666667 0.6666667 0.7000000 0.8888889
  [14] 0.8888889 0.7777778 0.5555556 0.7777778 0.8888889 0.5555556 0.6666667 0.6000000 0.7777778 1.0000000 0.6666667 0.6666667 0.7777778
  [27] 0.7777778 1.0000000 0.8888889 0.8888889 0.5555556 0.6666667 0.7777778 0.7777778 0.6666667 0.6666667 0.5555556 0.5555556 0.6666667
  [40] 0.6666667 0.5555556 0.7777778 0.5555556 0.6666667 1.0000000 1.0000000 0.5555556 0.6666667 0.5000000 0.5555556 0.7777778 0.8888889
  [53] 0.6666667 0.7777778 0.5555556 0.7777778 0.6666667 0.5555556 0.6666667 0.7000000 0.6666667 0.6666667 0.7777778 0.6666667 0.5555556
  [66] 0.6666667 0.7777778 0.8888889 1.0000000 1.0000000 0.5555556 0.5555556 0.7777778 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556
  [79] 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556 0.6666667 0.5555556 0.7777778 0.7777778
  [92] 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.6666667 0.7777778 0.5555556 0.5555556
 [105] 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556 0.7777778 0.6666667 0.5555556 0.7777778 0.6666667 0.7777778 1.0000000 0.7777778
 [118] 0.6666667 0.5000000 0.8888889 0.8888889 0.7777778 0.6666667 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.5555556 0.7777778
 [131] 0.7777778 0.7777778 0.5555556 0.7777778 0.6666667 1.0000000 0.8888889 0.5555556 0.6000000 0.5555556 0.7777778 0.8888889 0.8888889
 [144] 0.5555556 0.5555556 0.5555556 0.6666667 0.6666667 0.8888889 0.9000000 0.6666667 0.8888889 0.7777778 1.0000000 0.5555556 0.8888889
 [157] 0.6666667 0.5555556 0.7777778 1.0000000 0.8888889 0.8000000 0.5555556 0.6666667 0.6666667 0.8888889 0.8000000 0.6666667 0.6666667
 [170] 0.6666667 0.6666667 0.6666667 0.5555556 0.6666667 0.5555556 0.7777778 0.6666667 0.8888889 0.6666667 0.6000000 0.5555556 0.5555556
 [183] 0.6666667 0.7777778 0.7777778 0.5555556 0.5555556 0.8888889 0.7777778 0.6666667 0.6666667 0.7777778 0.6000000 0.5555556 0.6666667
 [196] 0.8888889 0.8888889 0.8888889 0.8888889 0.7000000 0.6666667 0.5555556 0.5555556 0.7777778 0.8888889 0.7777778 0.7777778 0.5555556
 [209] 0.6666667 0.6666667 0.7777778 0.6000000 0.7777778 0.5555556 0.5555556 0.6666667 0.8888889 0.7777778 0.7777778 0.5000000 0.6666667
 [222] 0.5555556 0.7777778 0.5555556 1.0000000 0.7777778 0.5555556 0.8888889 0.5000000 0.6666667 0.5000000 0.5555556 0.6666667 0.5555556
 [235] 0.6000000 0.7777778 0.6000000 0.5555556 0.8888889 0.8888889 0.5555556 0.6666667 0.8888889 0.7777778 1.0000000 0.8888889 0.7000000
 [248] 0.5555556 0.5555556 0.7777778 0.7777778 0.6666667 0.8000000 0.6666667 0.6666667 0.6666667 0.8888889 0.5555556 0.5000000 0.6666667
 [261] 0.7777778 0.6666667 0.7777778 0.6666667 0.5555556 0.6000000 0.5555556 0.7777778 0.5555556 0.7777778 0.7777778 1.0000000 0.6666667
 [274] 0.6666667 0.6666667 0.8888889 0.8888889 0.5555556 0.7777778 0.8888889 0.5555556 0.5555556 0.8888889 0.8888889 0.5555556 0.9000000
 [287] 0.6666667 0.6666667 0.5555556 0.6666667 0.6666667 0.5555556 0.8888889 0.6666667 0.5555556 0.5555556 0.5555556 0.5555556 0.6666667
 [300] 0.8888889 1.0000000 0.7777778 0.7777778 0.7777778 0.7777778 0.8888889 1.0000000 0.6666667 0.7000000 0.7777778 0.6666667 0.5555556
 [313] 0.5000000 0.6666667 0.5555556 0.5555556 0.5000000 0.8888889 1.0000000 1.0000000 0.6666667 0.8888889 0.5555556 0.8888889 0.8888889
 [326] 0.6666667 0.6666667 0.8888889 0.7777778 0.6666667 0.5555556 0.5555556 0.8000000 0.5555556 1.0000000 0.5555556 0.5555556 0.7777778
 [339] 1.0000000 0.8888889 0.7000000 0.5555556 0.6666667 0.5555556 0.5555556 0.7777778 0.5555556 0.8888889 0.5555556 0.6666667 0.6000000
 [352] 0.7777778 0.8000000 0.6666667 0.8888889 0.6666667 0.6000000 0.9000000 0.5000000 0.5555556 0.5000000 0.5555556 0.8888889 0.8888889
 [365] 0.5000000 0.5555556 1.0000000 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5000000 0.5555556 0.5555556 0.6666667 0.5555556
 [378] 0.5555556 0.6000000 0.6666667 0.5555556 0.7000000 0.8000000 0.6000000 0.6666667 0.7777778 0.6666667 0.6666667 0.6666667 0.5555556
 [391] 0.5555556 0.6000000 0.5555556 0.6666667 0.5555556 0.5555556 0.7777778 0.7000000 0.5555556 0.5555556 0.5555556 0.6666667 1.0000000
 [404] 1.0000000 0.8888889 0.7777778 0.6666667 0.5555556 0.5555556 0.8888889 0.6666667 0.7777778 0.5555556 0.5555556 0.6666667 0.7777778
 [417] 0.5555556 0.6666667 0.6000000 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.7777778 0.7777778 0.8888889 0.5555556 0.5555556
 [430] 0.5555556 0.5555556 0.8888889 0.7777778 0.7000000 0.6666667 0.7777778 0.8888889 0.5000000 0.7000000 0.7777778 0.7777778 0.6666667
 [443] 0.5555556 0.8888889 0.7777778 0.5000000 0.5555556 0.5555556 0.8888889 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.6666667
 [456] 0.7777778 0.6666667 0.6666667 0.6666667 0.7777778 0.8888889 0.5555556 0.6000000 0.5555556 0.5555556 0.6000000 0.7777778 1.0000000
 [469] 0.8888889 0.8888889 1.0000000 0.8888889 0.5555556 0.7777778 0.5555556 0.5555556 0.6666667 0.8888889 0.7000000 0.5555556 0.5555556
 [482] 0.7777778 1.0000000 1.0000000 0.6000000 0.5555556 0.7777778 0.8888889 1.0000000 0.7777778 0.5555556 0.7777778 0.6666667 0.6666667
 [495] 0.7777778 0.8888889 0.7777778 0.6666667 0.6666667 1.0000000 0.7777778 0.6666667 0.5555556 0.6666667 0.7777778 0.6666667 0.8888889
 [508] 0.8000000 0.9000000 0.7777778 0.5555556 0.6666667 0.6666667 0.8888889 0.6666667 0.7777778 0.5555556 0.6000000 0.8888889 0.7777778
 [521] 0.6666667 0.5555556 0.8000000 0.5555556 0.6666667 0.5555556 0.6666667 0.7000000 0.8888889 0.8888889 0.5555556 0.7777778 0.8000000
 [534] 0.5000000 0.5555556 0.6666667 0.6666667 0.5555556 0.7777778 0.8888889 0.7000000 1.0000000 1.0000000 0.5555556 0.5555556 0.5555556
 [547] 1.0000000 0.8888889 0.8888889 0.5555556 0.7777778 0.5555556 0.7777778 0.5555556 0.5555556 0.5555556 0.5555556 0.7777778 0.6666667
 [560] 0.5555556 0.8888889 0.8888889 0.8888889 0.5555556 0.7777778 0.8000000 0.8888889 0.7777778 0.5555556 0.7777778 0.7777778 0.5555556
 [573] 0.5555556 1.0000000 0.7777778 0.6666667 1.0000000 0.5555556 1.0000000 1.0000000 0.5555556 0.5555556 0.6666667 0.5555556 0.6000000
 [586] 0.5555556 1.0000000 0.5555556 0.6666667 0.5555556 0.5555556 0.8888889 0.6666667 0.6666667 0.5555556 1.0000000 1.0000000 0.5555556
 [599] 0.5555556 0.6666667 0.5555556 0.7777778 0.7777778 0.5555556 0.6666667 1.0000000 0.7777778 0.5555556 0.6666667 0.8888889 1.0000000
 [612] 0.6000000 0.7777778 0.5555556 0.5555556 0.5555556 0.6666667 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556
 [625] 0.7000000 1.0000000 0.8888889 0.6000000 0.5555556 0.6666667 0.8888889 0.7777778 0.5555556 0.5555556 0.6666667 0.7777778 1.0000000
 [638] 0.5555556 0.6666667 0.7777778 0.8888889 1.0000000 0.5555556 0.5555556 0.5555556 0.8888889 0.7777778 0.5555556 0.7777778 0.7000000
 [651] 0.6666667 0.7777778 0.5555556 0.6666667 0.5555556 0.6666667 0.6666667 0.5000000 0.7777778 0.8888889 0.7000000 0.5000000 0.5555556
 [664] 0.8888889 0.5555556 0.5555556 0.5555556 0.8888889 0.6666667 0.5555556 0.5555556 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556
 [677] 0.6666667 0.8888889 0.8888889 0.7777778 0.5555556 0.5555556 0.6666667 0.6000000 0.5555556 0.7777778 0.5555556 0.5555556 0.6666667
 [690] 0.7777778 0.7000000 0.6666667 0.5555556 0.5555556 0.6666667 0.5555556 0.6666667 0.6666667 1.0000000 0.6666667 0.8000000 1.0000000
 [703] 0.5555556 0.6666667 0.5555556 0.8888889 1.0000000 0.6666667 0.7777778 0.6666667 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556
 [716] 0.5000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556 0.8000000 0.7000000 0.5555556 0.8000000 0.7777778 0.5555556
 [729] 0.8888889 0.5555556 0.6666667 0.5555556 0.6666667 0.7777778 0.6000000 0.5555556 0.6000000 0.6666667 0.6666667 1.0000000 1.0000000
 [742] 0.8888889 0.6666667 0.6000000 1.0000000 0.6666667 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.8888889 0.5555556 0.5555556
 [755] 0.6000000 0.8888889 0.8888889 0.5555556 0.5555556 0.5555556 0.5000000 0.5555556 0.5000000 0.6666667 0.6666667 0.8000000 0.8888889
 [768] 1.0000000 1.0000000 0.5555556 0.5555556 0.7777778 0.5555556 0.5555556 0.5555556 0.7777778 0.7777778 0.6666667 0.8888889 0.6666667
 [781] 0.5555556 0.6000000 0.7777778 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.7777778 0.5555556 1.0000000 0.7000000 0.5555556
 [794] 0.5555556 0.6666667 0.5555556 0.8888889 0.5555556 0.6666667 0.6666667 0.8888889 0.7000000 0.6666667 0.5555556 0.6000000 0.5555556
 [807] 0.6000000 0.8888889 0.7777778 0.5555556 0.6666667 0.6666667 0.8888889 0.7000000 0.7777778 0.8888889 0.8888889 0.8888889 0.6666667
 [820] 0.7777778 0.5555556 0.8000000 0.7777778 0.6666667 0.5000000 0.7000000 0.8888889 0.6666667 0.6000000 0.8888889 0.8888889 0.8888889
 [833] 0.5555556 0.5555556 0.7777778 0.7000000 0.6666667 0.5555556 0.7777778 0.8888889 0.6666667 0.6666667 0.6666667 0.6000000 0.6000000
 [846] 0.6666667 0.6666667 0.8888889 0.8888889 0.5555556 0.5555556 0.8888889 0.7777778 0.7777778 0.5555556 0.6666667 0.7777778 0.8888889
 [859] 0.7777778 0.5555556 0.7000000 0.5555556 0.6666667 0.5555556 0.6000000 0.5555556 0.6666667 0.6666667 0.5555556 0.6666667 0.6666667
 [872] 0.5555556 0.6000000 0.8888889 0.6666667 0.6666667 1.0000000 0.7777778 0.5555556 0.8888889 0.5555556 0.5555556 0.5555556 0.8888889
 [885] 0.6000000 0.6666667 0.6666667 0.8888889 0.6666667 0.6666667 0.8888889 0.8888889 0.6000000 0.5555556 0.6666667 0.6666667 0.6666667
 [898] 0.5555556 0.8888889 0.6666667 0.6000000 0.5555556 0.5555556 0.7777778 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556 0.6000000
 [911] 0.6000000 0.6000000 0.5555556 0.8000000 0.8000000 0.6666667 0.6666667 0.6666667 0.5000000 0.5555556 0.6000000 0.5555556 0.8888889
 [924] 0.7777778 0.6666667 0.5555556 0.5555556 0.9000000 1.0000000 0.8888889 0.5000000 0.5555556 0.5000000 0.7777778 0.6666667 0.8888889
 [937] 0.5555556 0.5555556 0.6666667 0.7777778 0.5555556 0.6666667 0.6000000 0.6000000 0.6666667 0.6666667 1.0000000 1.0000000 0.7000000
 [950] 0.6666667 0.5000000 0.5555556 0.6000000 0.7777778 0.5555556 0.6666667 0.7777778 0.5555556 0.5555556 0.5555556 0.5555556 0.5555556
 [963] 0.6666667 0.5555556 0.5555556 0.6666667 0.6666667 0.8888889 0.6666667 0.5555556 0.5555556 0.7777778 0.6666667 0.7777778 0.5555556
 [976] 0.6666667 0.5555556 0.7777778 0.8888889 1.0000000 0.6666667 0.6666667 0.6000000 1.0000000 0.5555556 0.8000000 0.7777778 0.7777778
 [989] 0.7000000 0.6666667 0.8000000 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556 0.9000000 0.5555556 0.6666667 0.6000000
 [ reached getOption("max.print") -- omitted 268463 entries ]
> 
> roc.model_knn = pROC::roc(
+   testData$did_crash_happen, 
+   attributes(model_knn)$prob
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_knn = pROC::auc(roc.model_knn)
> print(auc.model_knn)
Area under the curve: 0.4743
> 
> #plot ROC curve
> plot.roc(roc.model_knn, print.auc = TRUE, col = 'red' , print.thres = "best" )
> 
> confusionMatrix(model_knn, testData_response_column)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  123272  22988
       yes  81802  41401
                                         
               Accuracy : 0.6111         
                 95% CI : (0.6093, 0.613)
    No Information Rate : 0.761          
    P-Value [Acc > NIR] : 1              
                                         
                  Kappa : 0.1859         
                                         
 Mcnemar's Test P-Value : <2e-16         
                                         
            Sensitivity : 0.6011         
            Specificity : 0.6430         
         Pos Pred Value : 0.8428         
         Neg Pred Value : 0.3360         
             Prevalence : 0.7610         
         Detection Rate : 0.4575         
   Detection Prevalence : 0.5428         
      Balanced Accuracy : 0.6220         
                                         
       'Positive' Class : no             
                                         
> library(dplyr)
> library(gbm)
> library(caTools)
> library(pROC)
> library(doParallel)
> library(caret)
> #library(DMwR)
> #library(ROSE)
> library(MLmetrics)
> library(class)
> 
> motor_collision_crash_clean_data <- read.csv("C:\\Users\\alexf\\Documents\\CSML1000\\Assignments\\CSML1000-Group-10-assignment-1\\data\\motor_vehicle_collisions_crashes_cleaned.csv")
> 
> #data <- select(motor_collision_crash_clean_data, -c(id, timestamp, total_number_of_crashes))
> data <- select(motor_collision_crash_clean_data, -c(id, total_number_of_crashes))
> 
> data$timestamp = as.Date(data$timestamp)
> data$precinct = as.numeric(as.factor(data$precinct))
> data$month = as.numeric(data$month)
> data$week = as.numeric(data$week)
> data$day = as.numeric(data$day)
> data$weekday = as.numeric(data$weekday)
> data$hour = as.numeric(data$hour)
> data$did_crash_happen = as.factor(
+   ifelse(data$did_crash_happen ==  0, "no", "yes")
+ )
> 
> set.seed(123)
> data_newer = data[data$timestamp > '2018-01-27', ]
> data_sample = sample.split(data_newer$hour,SplitRatio=0.80)
> trainData = subset(data_newer, data_sample==TRUE)
> testData = subset(data_newer, data_sample==FALSE)
> 
> trainData <- select(trainData, -c(timestamp))
> testData <- select(testData, -c (timestamp))
> 
> set.seed(123)
> columns = colnames(trainData)
> trainData_upsampled = upSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_upsampled$did_crash_happen))

    no    yes 
821202 821202 
> 
> #try downsampling instead...
> trainData_downsampled = downSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_downsampled$did_crash_happen))

    no    yes 
256604 256604 
> 
> #for KNN, we need to separate train response variable so it is put into cl 
> trainData_downsampled_response_column = trainData_downsampled[, ncol(trainData_downsampled)] #last column should always be response variable
> testData_response_column = testData[, ncol(testData)]
> 
> #remove response variable from trainData and testData
> #and normalize the predictors
> nor <- function(x) { (x -min(x))/(max(x)-min(x))   }
> trainData_downsampled_without_response_var = as.data.frame(lapply(trainData_downsampled[, 1:ncol(trainData_downsampled) - 1], nor))
> testData_without_response_var = as.data.frame(lapply(testData[, 1:ncol(testData) - 1], nor))
> 
> #PART 2: FOR LIBRARY-KNN ONLY
> #remove unnecessary data objects for library-knn
> rm(motor_collision_crash_clean_data, data, data_newer, data_sample, trainData)
> rm(trainData_upsampled)
> #rm(trainData_downsampled, testData)
> gc()
           used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells  2411887 128.9    7666308  409.5   7596604  405.8
Vcells 24636286 188.0  180021869 1373.5 252479278 1926.3
> 
> set.seed(123)
> ptm_rf <- proc.time()
> model_knn <- knn(
+   trainData_downsampled_without_response_var,
+   testData_without_response_var,
+   cl = trainData_downsampled_response_column,
+   k=9,
+   prob = TRUE #create probabilities so we can plot ROC
+ )
> proc.time() - ptm_rf
   user  system elapsed 
 629.81    0.07  631.98 
> 
> #probabilities 
> attributes(model_knn)$prob
   [1] 0.7777778 0.8000000 0.5555556 0.8888889 1.0000000 0.5555556 0.7777778 0.5555556 0.8888889 0.6666667 0.6666667 0.7000000 0.8888889
  [14] 0.8888889 0.7777778 0.5555556 0.7777778 0.8888889 0.5555556 0.6666667 0.6000000 0.7777778 1.0000000 0.6666667 0.6666667 0.7777778
  [27] 0.7777778 1.0000000 0.8888889 0.8888889 0.5555556 0.6666667 0.7777778 0.7777778 0.6666667 0.6666667 0.5555556 0.5555556 0.6666667
  [40] 0.6666667 0.5555556 0.7777778 0.5555556 0.6666667 1.0000000 1.0000000 0.5555556 0.6666667 0.5000000 0.5555556 0.7777778 0.8888889
  [53] 0.6666667 0.7777778 0.5555556 0.7777778 0.6666667 0.5555556 0.6666667 0.7000000 0.6666667 0.6666667 0.7777778 0.6666667 0.5555556
  [66] 0.6666667 0.7777778 0.8888889 1.0000000 1.0000000 0.5555556 0.5555556 0.7777778 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556
  [79] 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556 0.6666667 0.5555556 0.7777778 0.7777778
  [92] 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.6666667 0.7777778 0.5555556 0.5555556
 [105] 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556 0.7777778 0.6666667 0.5555556 0.7777778 0.6666667 0.7777778 1.0000000 0.7777778
 [118] 0.6666667 0.5000000 0.8888889 0.8888889 0.7777778 0.6666667 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.5555556 0.7777778
 [131] 0.7777778 0.7777778 0.5555556 0.7777778 0.6666667 1.0000000 0.8888889 0.5555556 0.6000000 0.5555556 0.7777778 0.8888889 0.8888889
 [144] 0.5555556 0.5555556 0.5555556 0.6666667 0.6666667 0.8888889 0.9000000 0.6666667 0.8888889 0.7777778 1.0000000 0.5555556 0.8888889
 [157] 0.6666667 0.5555556 0.7777778 1.0000000 0.8888889 0.8000000 0.5555556 0.6666667 0.6666667 0.8888889 0.8000000 0.6666667 0.6666667
 [170] 0.6666667 0.6666667 0.6666667 0.5555556 0.6666667 0.5555556 0.7777778 0.6666667 0.8888889 0.6666667 0.6000000 0.5555556 0.5555556
 [183] 0.6666667 0.7777778 0.7777778 0.5555556 0.5555556 0.8888889 0.7777778 0.6666667 0.6666667 0.7777778 0.6000000 0.5555556 0.6666667
 [196] 0.8888889 0.8888889 0.8888889 0.8888889 0.7000000 0.6666667 0.5555556 0.5555556 0.7777778 0.8888889 0.7777778 0.7777778 0.5555556
 [209] 0.6666667 0.6666667 0.7777778 0.6000000 0.7777778 0.5555556 0.5555556 0.6666667 0.8888889 0.7777778 0.7777778 0.5000000 0.6666667
 [222] 0.5555556 0.7777778 0.5555556 1.0000000 0.7777778 0.5555556 0.8888889 0.5000000 0.6666667 0.5000000 0.5555556 0.6666667 0.5555556
 [235] 0.6000000 0.7777778 0.6000000 0.5555556 0.8888889 0.8888889 0.5555556 0.6666667 0.8888889 0.7777778 1.0000000 0.8888889 0.7000000
 [248] 0.5555556 0.5555556 0.7777778 0.7777778 0.6666667 0.8000000 0.6666667 0.6666667 0.6666667 0.8888889 0.5555556 0.5000000 0.6666667
 [261] 0.7777778 0.6666667 0.7777778 0.6666667 0.5555556 0.6000000 0.5555556 0.7777778 0.5555556 0.7777778 0.7777778 1.0000000 0.6666667
 [274] 0.6666667 0.6666667 0.8888889 0.8888889 0.5555556 0.7777778 0.8888889 0.5555556 0.5555556 0.8888889 0.8888889 0.5555556 0.9000000
 [287] 0.6666667 0.6666667 0.5555556 0.6666667 0.6666667 0.5555556 0.8888889 0.6666667 0.5555556 0.5555556 0.5555556 0.5555556 0.6666667
 [300] 0.8888889 1.0000000 0.7777778 0.7777778 0.7777778 0.7777778 0.8888889 1.0000000 0.6666667 0.7000000 0.7777778 0.6666667 0.5555556
 [313] 0.5000000 0.6666667 0.5555556 0.5555556 0.5000000 0.8888889 1.0000000 1.0000000 0.6666667 0.8888889 0.5555556 0.8888889 0.8888889
 [326] 0.6666667 0.6666667 0.8888889 0.7777778 0.6666667 0.5555556 0.5555556 0.8000000 0.5555556 1.0000000 0.5555556 0.5555556 0.7777778
 [339] 1.0000000 0.8888889 0.7000000 0.5555556 0.6666667 0.5555556 0.5555556 0.7777778 0.5555556 0.8888889 0.5555556 0.6666667 0.6000000
 [352] 0.7777778 0.8000000 0.6666667 0.8888889 0.6666667 0.6000000 0.9000000 0.5000000 0.5555556 0.5000000 0.5555556 0.8888889 0.8888889
 [365] 0.5000000 0.5555556 1.0000000 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5000000 0.5555556 0.5555556 0.6666667 0.5555556
 [378] 0.5555556 0.6000000 0.6666667 0.5555556 0.7000000 0.8000000 0.6000000 0.6666667 0.7777778 0.6666667 0.6666667 0.6666667 0.5555556
 [391] 0.5555556 0.6000000 0.5555556 0.6666667 0.5555556 0.5555556 0.7777778 0.7000000 0.5555556 0.5555556 0.5555556 0.6666667 1.0000000
 [404] 1.0000000 0.8888889 0.7777778 0.6666667 0.5555556 0.5555556 0.8888889 0.6666667 0.7777778 0.5555556 0.5555556 0.6666667 0.7777778
 [417] 0.5555556 0.6666667 0.6000000 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.7777778 0.7777778 0.8888889 0.5555556 0.5555556
 [430] 0.5555556 0.5555556 0.8888889 0.7777778 0.7000000 0.6666667 0.7777778 0.8888889 0.5000000 0.7000000 0.7777778 0.7777778 0.6666667
 [443] 0.5555556 0.8888889 0.7777778 0.5000000 0.5555556 0.5555556 0.8888889 0.6666667 0.6666667 0.5555556 0.5555556 0.5555556 0.6666667
 [456] 0.7777778 0.6666667 0.6666667 0.6666667 0.7777778 0.8888889 0.5555556 0.6000000 0.5555556 0.5555556 0.6000000 0.7777778 1.0000000
 [469] 0.8888889 0.8888889 1.0000000 0.8888889 0.5555556 0.7777778 0.5555556 0.5555556 0.6666667 0.8888889 0.7000000 0.5555556 0.5555556
 [482] 0.7777778 1.0000000 1.0000000 0.6000000 0.5555556 0.7777778 0.8888889 1.0000000 0.7777778 0.5555556 0.7777778 0.6666667 0.6666667
 [495] 0.7777778 0.8888889 0.7777778 0.6666667 0.6666667 1.0000000 0.7777778 0.6666667 0.5555556 0.6666667 0.7777778 0.6666667 0.8888889
 [508] 0.8000000 0.9000000 0.7777778 0.5555556 0.6666667 0.6666667 0.8888889 0.6666667 0.7777778 0.5555556 0.6000000 0.8888889 0.7777778
 [521] 0.6666667 0.5555556 0.8000000 0.5555556 0.6666667 0.5555556 0.6666667 0.7000000 0.8888889 0.8888889 0.5555556 0.7777778 0.8000000
 [534] 0.5000000 0.5555556 0.6666667 0.6666667 0.5555556 0.7777778 0.8888889 0.7000000 1.0000000 1.0000000 0.5555556 0.5555556 0.5555556
 [547] 1.0000000 0.8888889 0.8888889 0.5555556 0.7777778 0.5555556 0.7777778 0.5555556 0.5555556 0.5555556 0.5555556 0.7777778 0.6666667
 [560] 0.5555556 0.8888889 0.8888889 0.8888889 0.5555556 0.7777778 0.8000000 0.8888889 0.7777778 0.5555556 0.7777778 0.7777778 0.5555556
 [573] 0.5555556 1.0000000 0.7777778 0.6666667 1.0000000 0.5555556 1.0000000 1.0000000 0.5555556 0.5555556 0.6666667 0.5555556 0.6000000
 [586] 0.5555556 1.0000000 0.5555556 0.6666667 0.5555556 0.5555556 0.8888889 0.6666667 0.6666667 0.5555556 1.0000000 1.0000000 0.5555556
 [599] 0.5555556 0.6666667 0.5555556 0.7777778 0.7777778 0.5555556 0.6666667 1.0000000 0.7777778 0.5555556 0.6666667 0.8888889 1.0000000
 [612] 0.6000000 0.7777778 0.5555556 0.5555556 0.5555556 0.6666667 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556
 [625] 0.7000000 1.0000000 0.8888889 0.6000000 0.5555556 0.6666667 0.8888889 0.7777778 0.5555556 0.5555556 0.6666667 0.7777778 1.0000000
 [638] 0.5555556 0.6666667 0.7777778 0.8888889 1.0000000 0.5555556 0.5555556 0.5555556 0.8888889 0.7777778 0.5555556 0.7777778 0.7000000
 [651] 0.6666667 0.7777778 0.5555556 0.6666667 0.5555556 0.6666667 0.6666667 0.5000000 0.7777778 0.8888889 0.7000000 0.5000000 0.5555556
 [664] 0.8888889 0.5555556 0.5555556 0.5555556 0.8888889 0.6666667 0.5555556 0.5555556 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556
 [677] 0.6666667 0.8888889 0.8888889 0.7777778 0.5555556 0.5555556 0.6666667 0.6000000 0.5555556 0.7777778 0.5555556 0.5555556 0.6666667
 [690] 0.7777778 0.7000000 0.6666667 0.5555556 0.5555556 0.6666667 0.5555556 0.6666667 0.6666667 1.0000000 0.6666667 0.8000000 1.0000000
 [703] 0.5555556 0.6666667 0.5555556 0.8888889 1.0000000 0.6666667 0.7777778 0.6666667 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556
 [716] 0.5000000 0.5555556 0.5555556 0.6666667 0.5555556 0.5555556 0.5555556 0.8000000 0.7000000 0.5555556 0.8000000 0.7777778 0.5555556
 [729] 0.8888889 0.5555556 0.6666667 0.5555556 0.6666667 0.7777778 0.6000000 0.5555556 0.6000000 0.6666667 0.6666667 1.0000000 1.0000000
 [742] 0.8888889 0.6666667 0.6000000 1.0000000 0.6666667 0.6666667 0.5555556 0.7777778 0.6666667 0.5555556 0.8888889 0.5555556 0.5555556
 [755] 0.6000000 0.8888889 0.8888889 0.5555556 0.5555556 0.5555556 0.5000000 0.5555556 0.5000000 0.6666667 0.6666667 0.8000000 0.8888889
 [768] 1.0000000 1.0000000 0.5555556 0.5555556 0.7777778 0.5555556 0.5555556 0.5555556 0.7777778 0.7777778 0.6666667 0.8888889 0.6666667
 [781] 0.5555556 0.6000000 0.7777778 0.6000000 0.5555556 0.5555556 0.6666667 0.5555556 0.7777778 0.5555556 1.0000000 0.7000000 0.5555556
 [794] 0.5555556 0.6666667 0.5555556 0.8888889 0.5555556 0.6666667 0.6666667 0.8888889 0.7000000 0.6666667 0.5555556 0.6000000 0.5555556
 [807] 0.6000000 0.8888889 0.7777778 0.5555556 0.6666667 0.6666667 0.8888889 0.7000000 0.7777778 0.8888889 0.8888889 0.8888889 0.6666667
 [820] 0.7777778 0.5555556 0.8000000 0.7777778 0.6666667 0.5000000 0.7000000 0.8888889 0.6666667 0.6000000 0.8888889 0.8888889 0.8888889
 [833] 0.5555556 0.5555556 0.7777778 0.7000000 0.6666667 0.5555556 0.7777778 0.8888889 0.6666667 0.6666667 0.6666667 0.6000000 0.6000000
 [846] 0.6666667 0.6666667 0.8888889 0.8888889 0.5555556 0.5555556 0.8888889 0.7777778 0.7777778 0.5555556 0.6666667 0.7777778 0.8888889
 [859] 0.7777778 0.5555556 0.7000000 0.5555556 0.6666667 0.5555556 0.6000000 0.5555556 0.6666667 0.6666667 0.5555556 0.6666667 0.6666667
 [872] 0.5555556 0.6000000 0.8888889 0.6666667 0.6666667 1.0000000 0.7777778 0.5555556 0.8888889 0.5555556 0.5555556 0.5555556 0.8888889
 [885] 0.6000000 0.6666667 0.6666667 0.8888889 0.6666667 0.6666667 0.8888889 0.8888889 0.6000000 0.5555556 0.6666667 0.6666667 0.6666667
 [898] 0.5555556 0.8888889 0.6666667 0.6000000 0.5555556 0.5555556 0.7777778 0.5555556 0.6666667 0.6666667 0.5555556 0.5555556 0.6000000
 [911] 0.6000000 0.6000000 0.5555556 0.8000000 0.8000000 0.6666667 0.6666667 0.6666667 0.5000000 0.5555556 0.6000000 0.5555556 0.8888889
 [924] 0.7777778 0.6666667 0.5555556 0.5555556 0.9000000 1.0000000 0.8888889 0.5000000 0.5555556 0.5000000 0.7777778 0.6666667 0.8888889
 [937] 0.5555556 0.5555556 0.6666667 0.7777778 0.5555556 0.6666667 0.6000000 0.6000000 0.6666667 0.6666667 1.0000000 1.0000000 0.7000000
 [950] 0.6666667 0.5000000 0.5555556 0.6000000 0.7777778 0.5555556 0.6666667 0.7777778 0.5555556 0.5555556 0.5555556 0.5555556 0.5555556
 [963] 0.6666667 0.5555556 0.5555556 0.6666667 0.6666667 0.8888889 0.6666667 0.5555556 0.5555556 0.7777778 0.6666667 0.7777778 0.5555556
 [976] 0.6666667 0.5555556 0.7777778 0.8888889 1.0000000 0.6666667 0.6666667 0.6000000 1.0000000 0.5555556 0.8000000 0.7777778 0.7777778
 [989] 0.7000000 0.6666667 0.8000000 0.7777778 0.7777778 0.5555556 0.5555556 0.5555556 0.9000000 0.5555556 0.6666667 0.6000000
 [ reached getOption("max.print") -- omitted 268463 entries ]
> 
> roc.model_knn = pROC::roc(
+   testData$did_crash_happen, 
+   attributes(model_knn)$prob
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_knn = pROC::auc(roc.model_knn)
> print(auc.model_knn)
Area under the curve: 0.4743
> 
> #plot ROC curve
> plot.roc(roc.model_knn, print.auc = TRUE, col = 'red' , print.thres = "best" )
> 
> confusionMatrix(model_knn, testData_response_column)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  123272  22988
       yes  81802  41401
                                         
               Accuracy : 0.6111         
                 95% CI : (0.6093, 0.613)
    No Information Rate : 0.761          
    P-Value [Acc > NIR] : 1              
                                         
                  Kappa : 0.1859         
                                         
 Mcnemar's Test P-Value : <2e-16         
                                         
            Sensitivity : 0.6011         
            Specificity : 0.6430         
         Pos Pred Value : 0.8428         
         Neg Pred Value : 0.3360         
             Prevalence : 0.7610         
         Detection Rate : 0.4575         
   Detection Prevalence : 0.5428         
      Balanced Accuracy : 0.6220         
                                         
       'Positive' Class : no             
                                         
> library(dplyr)
> library(gbm)
> library(caTools)
> library(pROC)
> library(doParallel)
> library(caret)
> #library(DMwR)
> #library(ROSE)
> library(MLmetrics)
> library(class)
> 
> motor_collision_crash_clean_data <- read.csv("C:\\Users\\alexf\\Documents\\CSML1000\\Assignments\\CSML1000-Group-10-assignment-1\\data\\motor_vehicle_collisions_crashes_cleaned.csv")
> 
> #data <- select(motor_collision_crash_clean_data, -c(id, timestamp, total_number_of_crashes))
> data <- select(motor_collision_crash_clean_data, -c(id, total_number_of_crashes))
> 
> data$timestamp = as.Date(data$timestamp)
> data$precinct = as.numeric(as.factor(data$precinct))
> data$month = as.numeric(data$month)
> data$week = as.numeric(data$week)
> data$day = as.numeric(data$day)
> data$weekday = as.numeric(data$weekday)
> data$hour = as.numeric(data$hour)
> data$did_crash_happen = as.factor(
+   ifelse(data$did_crash_happen ==  0, "no", "yes")
+ )
> 
> set.seed(123)
> data_newer = data[data$timestamp > '2017-01-27', ]
> data_sample = sample.split(data_newer$hour,SplitRatio=0.80)
> trainData = subset(data_newer, data_sample==TRUE)
> testData = subset(data_newer, data_sample==FALSE)
> 
> trainData <- select(trainData, -c(timestamp))
> testData <- select(testData, -c (timestamp))
> 
> set.seed(123)
> columns = colnames(trainData)
> trainData_upsampled = upSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_upsampled$did_crash_happen))

     no     yes 
1225241 1225241 
> 
> #try downsampling instead...
> trainData_downsampled = downSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_downsampled$did_crash_happen))

    no    yes 
392181 392181 
> 
> #for KNN, we need to separate train response variable so it is put into cl 
> trainData_downsampled_response_column = trainData_downsampled[, ncol(trainData_downsampled)] #last column should always be response variable
> testData_response_column = testData[, ncol(testData)]
> 
> #remove response variable from trainData and testData
> #and normalize the predictors
> nor <- function(x) { (x -min(x))/(max(x)-min(x))   }
> trainData_downsampled_without_response_var = as.data.frame(lapply(trainData_downsampled[, 1:ncol(trainData_downsampled) - 1], nor))
> testData_without_response_var = as.data.frame(lapply(testData[, 1:ncol(testData) - 1], nor))
> 
> #PART 2: FOR LIBRARY-KNN ONLY
> #remove unnecessary data objects for library-knn
> rm(motor_collision_crash_clean_data, data, data_newer, data_sample, trainData)
> rm(trainData_upsampled)
> #rm(trainData_downsampled, testData)
> gc()
           used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells  2412040 128.9    7666308  409.5   7602314  406.1
Vcells 29779589 227.3  172884994 1319.1 252479278 1926.3
> 
> set.seed(123)
> ptm_rf <- proc.time()
> model_knn <- knn(
+   trainData_downsampled_without_response_var,
+   testData_without_response_var,
+   cl = trainData_downsampled_response_column,
+   k=11,
+   prob = TRUE #create probabilities so we can plot ROC
+ )
> proc.time() - ptm_rf
   user  system elapsed 
1432.18    0.22 1438.08 
> 
> #probabilities 
> attributes(model_knn)$prob
   [1] 0.6363636 0.5454545 0.5454545 0.8181818 0.5454545 0.6363636 0.8181818 0.8181818 0.7500000 0.6363636 0.7272727 0.9090909 0.7272727
  [14] 0.6363636 0.6363636 0.8181818 0.5454545 0.8181818 0.7272727 0.6363636 0.5454545 0.7272727 0.8333333 0.5454545 0.5454545 0.6363636
  [27] 0.6363636 0.7272727 1.0000000 0.5454545 0.6363636 0.6363636 0.7272727 0.8181818 0.8181818 0.6363636 0.5000000 0.5454545 0.5454545
  [40] 0.5454545 0.8181818 0.7272727 0.7272727 0.7272727 0.5454545 0.5454545 0.7272727 1.0000000 0.8181818 0.5454545 0.5454545 0.8181818
  [53] 0.5454545 0.5833333 0.6363636 0.9166667 0.5454545 0.5454545 0.5454545 0.9090909 1.0000000 0.8181818 0.7272727 0.5454545 0.7272727
  [66] 1.0000000 1.0000000 0.7272727 0.5454545 0.8181818 0.5833333 0.7272727 0.7272727 0.8181818 0.5454545 0.7500000 0.5454545 0.8181818
  [79] 0.5454545 0.6363636 0.5454545 0.8181818 0.8181818 0.5454545 0.6363636 0.6363636 0.8181818 0.5833333 0.5454545 0.6363636 0.6363636
  [92] 0.6363636 0.5454545 0.8181818 0.5454545 0.8181818 0.7272727 1.0000000 0.8181818 0.5454545 0.5454545 0.6363636 0.5454545 0.5454545
 [105] 0.5454545 0.5454545 0.6363636 0.8181818 0.7272727 0.6363636 0.5454545 0.5454545 0.5454545 0.7500000 0.9090909 0.7272727 0.6363636
 [118] 1.0000000 0.9090909 0.9090909 0.9090909 0.7272727 0.5454545 0.5454545 0.9090909 0.7272727 0.7272727 0.6363636 0.6363636 0.6363636
 [131] 0.5454545 0.5454545 0.8181818 0.9090909 0.5454545 0.5000000 0.7500000 0.6363636 0.5833333 0.6363636 0.5000000 0.8181818 1.0000000
 [144] 0.5000000 0.5454545 0.5454545 0.5454545 0.6363636 0.7272727 0.5454545 0.7272727 0.6363636 0.5833333 0.6363636 0.6363636 0.9090909
 [157] 0.7272727 0.7272727 0.5454545 0.6363636 0.7272727 0.9090909 0.8181818 0.8333333 0.7272727 0.7272727 0.6666667 0.5833333 0.7272727
 [170] 0.8181818 0.6666667 0.5454545 0.7272727 0.5454545 0.7272727 0.5454545 0.5454545 0.8181818 0.9090909 0.9090909 0.7272727 0.8181818
 [183] 0.6363636 0.5000000 0.6363636 0.9090909 0.8333333 0.6363636 0.6363636 0.8181818 0.8181818 0.6363636 0.5833333 0.9090909 1.0000000
 [196] 0.8181818 0.9090909 1.0000000 0.8181818 0.8181818 0.8181818 0.7272727 0.8181818 0.5833333 0.5833333 0.6363636 0.7272727 0.9166667
 [209] 0.5454545 0.9090909 0.6363636 0.6363636 0.6363636 0.7272727 0.5454545 0.5454545 0.7272727 0.7272727 0.8181818 0.6363636 0.5454545
 [222] 0.6363636 0.6363636 0.7272727 0.7272727 0.6363636 0.5454545 0.5454545 0.5454545 0.5454545 0.9090909 0.8181818 0.7272727 0.6666667
 [235] 0.5454545 0.6363636 0.7272727 0.8181818 0.7272727 0.5454545 0.6363636 0.8181818 1.0000000 0.5000000 0.6363636 0.9090909 0.6363636
 [248] 0.5833333 0.8181818 0.5454545 0.7272727 0.7272727 0.9090909 0.5454545 0.7272727 0.6363636 0.5454545 0.6363636 0.7272727 0.5833333
 [261] 0.7272727 0.5454545 0.5454545 0.6363636 0.8181818 0.9090909 0.5454545 0.6363636 0.6363636 0.5454545 0.5454545 0.8181818 0.9090909
 [274] 0.6666667 0.5000000 0.9090909 0.7272727 0.5454545 0.7272727 0.8181818 0.9090909 0.9090909 0.5000000 0.7272727 0.5454545 0.5454545
 [287] 0.6363636 0.6363636 0.8181818 0.6363636 0.7272727 0.9090909 1.0000000 0.7272727 0.6363636 0.6363636 0.5454545 0.6363636 0.5454545
 [300] 0.7272727 0.5454545 0.6363636 0.7272727 0.8181818 0.5454545 0.5000000 0.5454545 0.6363636 0.7272727 0.5454545 0.6666667 0.6363636
 [313] 0.5454545 0.5454545 0.6363636 0.5454545 0.6363636 1.0000000 0.9166667 0.5454545 0.6363636 0.5000000 0.5454545 0.5454545 0.7272727
 [326] 0.9090909 0.5454545 0.7500000 0.5454545 0.7272727 0.6363636 0.9090909 0.7272727 0.8333333 0.5454545 0.7272727 0.6363636 0.5454545
 [339] 0.7272727 0.5454545 0.5454545 0.5454545 0.7272727 0.7272727 0.6363636 0.8181818 0.5454545 0.5454545 0.6363636 0.6363636 0.5454545
 [352] 0.8181818 0.5454545 0.5454545 0.8181818 0.7272727 0.7272727 0.6363636 0.7272727 0.5454545 0.7500000 0.6363636 0.5454545 0.5454545
 [365] 1.0000000 0.5454545 0.5454545 0.8181818 0.7272727 0.8181818 0.8181818 0.7272727 0.5454545 0.5454545 0.6363636 0.5454545 0.7272727
 [378] 0.8181818 0.9090909 0.9090909 0.9090909 0.7272727 0.5454545 0.5454545 0.5454545 0.9090909 0.7272727 0.5454545 0.5454545 0.6363636
 [391] 0.7272727 0.5454545 0.6363636 0.7272727 0.5454545 0.6363636 0.7272727 0.9090909 0.7272727 0.5454545 0.6363636 0.5454545 0.7272727
 [404] 1.0000000 0.8181818 0.7272727 0.5454545 0.7272727 0.7272727 0.5454545 0.5454545 0.8333333 0.6363636 0.5454545 0.6666667 0.5833333
 [417] 0.5454545 0.6363636 0.5454545 0.5454545 0.6363636 0.7272727 0.5454545 0.7500000 0.6363636 0.5454545 0.7272727 0.9090909 0.6363636
 [430] 0.6363636 0.6666667 0.7272727 0.7272727 0.6363636 0.5454545 0.6363636 0.5454545 0.8181818 0.8181818 0.9090909 0.8181818 0.5454545
 [443] 0.5454545 0.6363636 0.9090909 0.8181818 0.6363636 0.5454545 0.8181818 0.5454545 0.8181818 0.7272727 0.7272727 0.5454545 0.6363636
 [456] 0.7272727 0.6363636 0.6363636 0.5454545 0.5000000 0.5454545 0.6363636 0.9090909 0.8181818 0.8181818 0.6363636 0.6363636 0.6363636
 [469] 0.6363636 0.8181818 0.7272727 0.5454545 0.8181818 0.5454545 0.5454545 0.9090909 0.9090909 0.9090909 0.6363636 0.6363636 0.7272727
 [482] 0.5454545 0.6363636 0.9090909 0.9090909 0.9090909 0.5000000 0.5454545 0.6363636 0.7272727 0.8181818 0.7272727 0.5833333 0.5833333
 [495] 0.6363636 0.6363636 0.7272727 0.6363636 0.6363636 0.5454545 0.6363636 0.7272727 0.6363636 0.5454545 0.7272727 0.7272727 0.6363636
 [508] 0.7272727 0.5454545 0.6363636 0.8181818 0.8333333 0.8333333 0.6363636 0.6363636 0.6363636 0.6363636 0.5000000 0.6363636 0.5454545
 [521] 0.6363636 0.5454545 0.6363636 0.7272727 0.8181818 0.7500000 0.9166667 0.9166667 0.8181818 0.7272727 0.5833333 0.5454545 0.9090909
 [534] 0.7272727 0.5454545 0.5454545 0.6363636 0.5454545 0.5454545 0.6363636 0.5454545 0.7272727 0.6363636 0.5833333 0.6363636 0.7500000
 [547] 0.5000000 0.6363636 0.7272727 0.7272727 0.7272727 0.9090909 0.5454545 0.5454545 0.6666667 0.7272727 0.5833333 0.8181818 0.5454545
 [560] 0.7272727 0.5000000 0.7272727 0.7272727 0.7272727 0.7272727 0.6363636 0.8181818 1.0000000 0.8181818 0.5454545 0.5454545 0.6363636
 [573] 0.8333333 0.7272727 0.5454545 0.5454545 0.6363636 0.5454545 1.0000000 0.7272727 0.5454545 0.5000000 0.8181818 0.7272727 0.5454545
 [586] 0.7272727 0.5454545 0.9090909 0.8181818 1.0000000 1.0000000 0.5454545 0.9166667 0.6666667 0.8181818 0.6666667 0.6363636 0.8181818
 [599] 0.9090909 0.9166667 0.6363636 0.6666667 0.8181818 0.5833333 0.5454545 0.5454545 0.6363636 0.8333333 0.7272727 0.6363636 0.5454545
 [612] 0.6666667 0.6666667 0.5833333 0.9090909 0.7272727 0.5454545 0.9090909 0.8181818 0.9090909 0.5454545 0.5454545 0.5454545 0.5454545
 [625] 0.5454545 0.5454545 0.7272727 0.5454545 0.7272727 0.7272727 0.5454545 0.5454545 0.5454545 0.5454545 0.5454545 0.6363636 0.6363636
 [638] 0.5454545 0.8181818 0.8181818 0.9090909 0.6363636 0.6363636 0.7272727 1.0000000 0.7272727 0.5454545 1.0000000 0.7272727 0.5454545
 [651] 0.8181818 0.9090909 0.7272727 0.6666667 0.7272727 0.7500000 0.8181818 0.6363636 0.6363636 0.6363636 0.5454545 0.7272727 0.6363636
 [664] 0.5454545 0.7272727 0.6666667 0.5454545 0.6363636 0.8333333 0.5454545 0.7272727 0.9090909 0.8181818 0.8333333 0.7272727 0.5454545
 [677] 0.6363636 0.5454545 0.6363636 0.5454545 0.9090909 0.5454545 0.6363636 0.8181818 0.9090909 0.7272727 0.8181818 0.8181818 0.5454545
 [690] 0.5454545 0.6363636 0.9090909 0.6363636 0.6666667 0.6363636 0.6363636 0.6363636 0.5454545 0.7272727 0.5833333 0.5454545 0.5454545
 [703] 0.8181818 0.9090909 0.5000000 0.6363636 0.7272727 0.6363636 0.5454545 0.7272727 0.9090909 0.5454545 0.6666667 0.6363636 0.5454545
 [716] 0.6666667 0.5833333 0.5454545 0.5454545 0.5454545 0.7272727 0.6363636 0.6666667 0.5454545 0.9090909 0.5454545 0.5454545 0.8181818
 [729] 0.8181818 0.7272727 0.5454545 0.5454545 0.5454545 0.5454545 1.0000000 0.8181818 0.7272727 0.6363636 0.8333333 0.8181818 0.6363636
 [742] 0.7272727 0.9090909 0.5454545 0.6363636 0.6363636 0.5454545 0.5454545 0.7272727 0.6363636 0.9090909 0.8181818 0.5454545 0.5454545
 [755] 0.8333333 0.7272727 0.6363636 0.7272727 0.7272727 1.0000000 1.0000000 0.5454545 0.7272727 0.7272727 0.7272727 0.7272727 0.6363636
 [768] 0.6363636 0.9166667 1.0000000 0.8181818 0.6363636 0.5833333 0.9090909 0.5454545 0.7272727 0.8181818 0.8181818 0.8181818 0.6363636
 [781] 0.6363636 0.5454545 0.8181818 0.9090909 0.5454545 0.5833333 0.6363636 0.6363636 0.7272727 0.6363636 0.5454545 0.5454545 0.5454545
 [794] 0.6363636 0.8181818 0.7272727 0.5454545 0.7272727 0.7272727 0.6363636 0.8181818 0.6363636 0.8181818 0.8181818 0.7272727 0.5454545
 [807] 0.5454545 0.5454545 0.6363636 0.7272727 0.9090909 0.6363636 0.6363636 0.5454545 0.5454545 0.7272727 0.6363636 0.9090909 0.7272727
 [820] 0.5454545 0.6363636 0.8181818 0.5454545 0.5454545 0.5454545 0.6363636 0.7272727 0.8181818 0.8181818 0.7272727 0.7272727 0.6363636
 [833] 0.5454545 0.7272727 0.9090909 0.7272727 0.6363636 0.5454545 0.7272727 0.7272727 0.5833333 0.5454545 0.5454545 0.7272727 0.8181818
 [846] 0.6363636 0.5454545 0.6363636 0.5454545 0.6666667 0.8181818 0.7500000 0.9090909 0.8181818 0.5454545 0.6363636 0.5454545 0.5454545
 [859] 0.7272727 0.6666667 0.6363636 0.5454545 0.5454545 0.5454545 0.7272727 0.9090909 0.8181818 0.7272727 0.5454545 0.5454545 0.5454545
 [872] 0.8181818 0.9090909 0.5454545 0.5454545 0.5454545 0.6363636 0.5454545 0.6363636 0.5454545 0.6666667 0.6363636 0.6363636 0.6363636
 [885] 0.5454545 0.8181818 0.5454545 0.5000000 0.5454545 0.6363636 0.6363636 0.8333333 0.9090909 0.9090909 0.9090909 0.6363636 1.0000000
 [898] 0.8181818 0.7272727 0.5454545 0.5454545 0.7272727 0.6363636 0.6363636 0.7272727 0.7272727 0.8181818 0.9090909 0.7272727 0.5454545
 [911] 0.7272727 0.5454545 0.7272727 0.8181818 0.5454545 0.8181818 0.6363636 0.9090909 0.9090909 0.8181818 0.5454545 0.5454545 0.5000000
 [924] 0.8181818 0.9090909 0.8181818 0.5454545 0.8181818 0.9090909 1.0000000 0.6363636 0.5454545 0.5454545 0.7500000 1.0000000 0.9090909
 [937] 0.5454545 0.5454545 0.5454545 0.8181818 1.0000000 0.5454545 0.6666667 0.5454545 0.5454545 0.7272727 0.6363636 0.5454545 0.6363636
 [950] 0.8181818 0.6363636 0.8181818 0.7272727 0.7272727 0.8333333 0.8181818 0.6363636 0.5454545 0.7272727 0.7272727 0.8181818 0.7272727
 [963] 0.7272727 0.6363636 0.5454545 0.6363636 0.6363636 0.9166667 1.0000000 0.8181818 0.5833333 0.6363636 0.6363636 0.5000000 0.6363636
 [976] 1.0000000 1.0000000 0.7272727 0.7272727 0.8181818 0.8181818 0.6363636 0.5454545 0.5000000 0.8181818 0.5454545 0.6363636 0.7272727
 [989] 0.6363636 0.7272727 0.5454545 0.5454545 0.8181818 0.6666667 0.5454545 0.5454545 0.6363636 0.7272727 0.5454545 0.6363636
 [ reached getOption("max.print") -- omitted 403367 entries ]
> 
> roc.model_knn = pROC::roc(
+   testData$did_crash_happen, 
+   attributes(model_knn)$prob
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_knn = pROC::auc(roc.model_knn)
> print(auc.model_knn)
Area under the curve: 0.4736
> 
> #plot ROC curve
> plot.roc(roc.model_knn, print.auc = TRUE, col = 'red' , print.thres = "best" )
> 
> confusionMatrix(model_knn, testData_response_column)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  183498  33906
       yes 122929  64034
                                          
               Accuracy : 0.6121          
                 95% CI : (0.6106, 0.6136)
    No Information Rate : 0.7578          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.193           
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.5988          
            Specificity : 0.6538          
         Pos Pred Value : 0.8440          
         Neg Pred Value : 0.3425          
             Prevalence : 0.7578          
         Detection Rate : 0.4538          
   Detection Prevalence : 0.5376          
      Balanced Accuracy : 0.6263          
                                          
       'Positive' Class : no              
                                          
> library(dplyr)
> library(gbm)
> library(caTools)
> library(pROC)
> library(doParallel)
> library(caret)
> #library(DMwR)
> #library(ROSE)
> library(MLmetrics)
> library(class)
> 
> motor_collision_crash_clean_data <- read.csv("C:\\Users\\alexf\\Documents\\CSML1000\\Assignments\\CSML1000-Group-10-assignment-1\\data\\motor_vehicle_collisions_crashes_cleaned.csv")
> 
> #data <- select(motor_collision_crash_clean_data, -c(id, timestamp, total_number_of_crashes))
> data <- select(motor_collision_crash_clean_data, -c(id, total_number_of_crashes))
> 
> data$timestamp = as.Date(data$timestamp)
> data$precinct = as.numeric(as.factor(data$precinct))
> data$month = as.numeric(data$month)
> data$week = as.numeric(data$week)
> data$day = as.numeric(data$day)
> data$weekday = as.numeric(data$weekday)
> data$hour = as.numeric(data$hour)
> data$did_crash_happen = as.factor(
+   ifelse(data$did_crash_happen ==  0, "no", "yes")
+ )
> 
> set.seed(123)
> data_newer = data[data$timestamp > '2017-01-27', ]
> data_sample = sample.split(data_newer$hour,SplitRatio=0.80)
> trainData = subset(data_newer, data_sample==TRUE)
> testData = subset(data_newer, data_sample==FALSE)
> 
> trainData <- select(trainData, -c(timestamp))
> testData <- select(testData, -c (timestamp))
> 
> set.seed(123)
> columns = colnames(trainData)
> trainData_upsampled = upSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_upsampled$did_crash_happen))

     no     yes 
1225241 1225241 
> 
> #try downsampling instead...
> trainData_downsampled = downSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_downsampled$did_crash_happen))

    no    yes 
392181 392181 
> 
> #for KNN, we need to separate train response variable so it is put into cl 
> trainData_downsampled_response_column = trainData_downsampled[, ncol(trainData_downsampled)] #last column should always be response variable
> testData_response_column = testData[, ncol(testData)]
> 
> #remove response variable from trainData and testData
> #and normalize the predictors
> nor <- function(x) { (x -min(x))/(max(x)-min(x))   }
> trainData_downsampled_without_response_var = as.data.frame(lapply(trainData_downsampled[, 1:ncol(trainData_downsampled) - 1], nor))
> testData_without_response_var = as.data.frame(lapply(testData[, 1:ncol(testData) - 1], nor))
> 
> #PART 2: FOR LIBRARY-KNN ONLY
> #remove unnecessary data objects for library-knn
> rm(motor_collision_crash_clean_data, data, data_newer, data_sample, trainData)
> rm(trainData_upsampled)
> #rm(trainData_downsampled, testData)
> gc()
           used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells  2412042 128.9    7391656  394.8   7602314  406.1
Vcells 30184457 230.3  199304312 1520.6 252479278 1926.3
> 
> set.seed(123)
> ptm_rf <- proc.time()
> model_knn <- knn(
+   trainData_downsampled_without_response_var,
+   testData_without_response_var,
+   cl = trainData_downsampled_response_column,
+   k=3,
+   prob = TRUE #create probabilities so we can plot ROC
+ )
> proc.time() - ptm_rf
   user  system elapsed 
1422.44    0.14 1428.28 
> 
> #probabilities 
> attributes(model_knn)$prob
   [1] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.7500000 1.0000000 0.5000000
  [14] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667
  [27] 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
  [40] 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667
  [53] 0.6666667 0.5000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.7500000 0.6666667
  [66] 1.0000000 1.0000000 0.6666667 1.0000000 0.7500000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000
  [79] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667
  [92] 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [105] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.5000000 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000
 [118] 1.0000000 1.0000000 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667
 [131] 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000
 [144] 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667
 [157] 1.0000000 0.6666667 1.0000000 0.5000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [170] 1.0000000 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000
 [183] 0.5000000 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000
 [196] 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [209] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 1.0000000 1.0000000 0.6666667 0.5000000
 [222] 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667
 [235] 1.0000000 1.0000000 0.5000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [248] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667
 [261] 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667
 [274] 0.7500000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667
 [287] 0.7500000 0.5000000 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667
 [300] 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.5000000 0.6666667 0.6666667
 [313] 0.6666667 0.6666667 0.6666667 0.5000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [326] 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000
 [339] 0.7500000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [352] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.5000000 0.6666667
 [365] 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.5000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [378] 0.6666667 1.0000000 1.0000000 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000
 [391] 0.6666667 0.6666667 1.0000000 1.0000000 0.5000000 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000
 [404] 1.0000000 1.0000000 1.0000000 0.7500000 1.0000000 0.6666667 0.5000000 0.6666667 0.7500000 0.6666667 0.6666667 0.7500000 0.6666667
 [417] 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667
 [430] 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667
 [443] 0.5000000 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 0.6666667 0.6666667 0.7500000 0.6666667 1.0000000 0.7500000 1.0000000
 [456] 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.5000000 1.0000000 0.6666667
 [469] 1.0000000 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
 [482] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
 [495] 0.6666667 1.0000000 0.7500000 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000
 [508] 0.6666667 0.5000000 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.5000000 1.0000000
 [521] 1.0000000 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 1.0000000 0.7500000 1.0000000 0.6666667 0.6666667 1.0000000
 [534] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.5000000
 [547] 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667
 [560] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.7500000 0.6666667 0.6666667
 [573] 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [586] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000
 [599] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.7500000 1.0000000 0.6666667 1.0000000 0.6666667 0.7500000 0.6666667
 [612] 0.6666667 0.6666667 0.5000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.7500000 1.0000000 0.6666667 1.0000000
 [625] 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667
 [638] 0.6666667 1.0000000 0.7500000 1.0000000 0.5000000 0.6666667 0.7500000 1.0000000 1.0000000 0.7500000 1.0000000 0.6666667 1.0000000
 [651] 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000 0.7500000 0.5000000 0.6666667 1.0000000 0.6666667 0.6666667
 [664] 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.5000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667
 [677] 1.0000000 0.7500000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 1.0000000 0.6666667
 [690] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667
 [703] 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.7500000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000
 [716] 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000
 [729] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.7500000 1.0000000 0.6666667 1.0000000
 [742] 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [755] 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000 0.7500000 0.6666667
 [768] 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667
 [781] 0.6666667 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.5000000 0.6666667 1.0000000 1.0000000 0.6666667
 [794] 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667
 [807] 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667
 [820] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000 1.0000000 1.0000000 0.6666667 0.6666667
 [833] 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000
 [846] 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667
 [859] 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000
 [872] 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [885] 0.7500000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.7500000 0.6666667 1.0000000
 [898] 1.0000000 1.0000000 0.5000000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667
 [911] 1.0000000 0.6666667 0.6666667 1.0000000 0.5000000 0.6666667 0.7500000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [924] 0.5000000 1.0000000 1.0000000 0.6666667 0.6666667 0.7500000 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
 [937] 0.7500000 0.6666667 0.6666667 1.0000000 1.0000000 0.5000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000
 [950] 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.7500000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000
 [963] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.7500000 0.6666667 1.0000000 1.0000000
 [976] 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.5000000 0.6666667 1.0000000
 [989] 0.7500000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000
 [ reached getOption("max.print") -- omitted 403367 entries ]
> 
> roc.model_knn = pROC::roc(
+   testData$did_crash_happen, 
+   attributes(model_knn)$prob
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_knn = pROC::auc(roc.model_knn)
> print(auc.model_knn)
Area under the curve: 0.4831
> 
> #plot ROC curve
> plot.roc(roc.model_knn, print.auc = TRUE, col = 'red' , print.thres = "best" )
> 
> confusionMatrix(model_knn, testData_response_column)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  179828  38370
       yes 126599  59570
                                          
               Accuracy : 0.592           
                 95% CI : (0.5905, 0.5935)
    No Information Rate : 0.7578          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1493          
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.5869          
            Specificity : 0.6082          
         Pos Pred Value : 0.8242          
         Neg Pred Value : 0.3200          
             Prevalence : 0.7578          
         Detection Rate : 0.4447          
   Detection Prevalence : 0.5396          
      Balanced Accuracy : 0.5975          
                                          
       'Positive' Class : no              
                                          
> library(dplyr)
> library(gbm)
> library(caTools)
> library(pROC)
> library(doParallel)
> library(caret)
> #library(DMwR)
> #library(ROSE)
> library(MLmetrics)
> library(class)
> 
> motor_collision_crash_clean_data <- read.csv("C:\\Users\\alexf\\Documents\\CSML1000\\Assignments\\CSML1000-Group-10-assignment-1\\data\\motor_vehicle_collisions_crashes_cleaned.csv")
> 
> #data <- select(motor_collision_crash_clean_data, -c(id, timestamp, total_number_of_crashes))
> data <- select(motor_collision_crash_clean_data, -c(id, total_number_of_crashes))
> 
> data$timestamp = as.Date(data$timestamp)
> data$precinct = as.numeric(as.factor(data$precinct))
> data$month = as.numeric(data$month)
> data$week = as.numeric(data$week)
> data$day = as.numeric(data$day)
> data$weekday = as.numeric(data$weekday)
> data$hour = as.numeric(data$hour)
> data$did_crash_happen = as.factor(
+   ifelse(data$did_crash_happen ==  0, "no", "yes")
+ )
> 
> set.seed(123)
> # data_newer = data[data$timestamp > '2016-01-27', ]
> # data_sample = sample.split(data_newer$hour,SplitRatio=0.80)
> # trainData = subset(data_newer, data_sample==TRUE)
> # testData = subset(data_newer, data_sample==FALSE)
> trainData = data[data$timestamp > '2017-01-27' & data$timestamp < '2019-01-27', ]
> testData = data[data$timestamp > '2019-01-26', ]
> 
> trainData <- select(trainData, -c(timestamp))
> testData <- select(testData, -c (timestamp))
> 
> set.seed(123)
> columns = colnames(trainData)
> trainData_upsampled = upSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_upsampled$did_crash_happen))

     no     yes 
1011095 1011095 
> 
> #try downsampling instead...
> trainData_downsampled = downSample(
+   x = trainData[, columns[columns != "did_crash_happen"] ], 
+   y = trainData$did_crash_happen, list = F, yname = "did_crash_happen"
+ )
> print(table(trainData_downsampled$did_crash_happen))

    no    yes 
336097 336097 
> 
> #for KNN, we need to separate train response variable so it is put into cl 
> trainData_downsampled_response_column = trainData_downsampled[, ncol(trainData_downsampled)] #last column should always be response variable
> testData_response_column = testData[, ncol(testData)]
> 
> #remove response variable from trainData and testData
> #and normalize the predictors
> nor <- function(x) { (x -min(x))/(max(x)-min(x))   }
> trainData_downsampled_without_response_var = as.data.frame(lapply(trainData_downsampled[, 1:ncol(trainData_downsampled) - 1], nor))
> testData_without_response_var = as.data.frame(lapply(testData[, 1:ncol(testData) - 1], nor))
> 
> #PART 2: FOR LIBRARY-KNN ONLY
> #remove unnecessary data objects for library-knn
> rm(motor_collision_crash_clean_data, data, data_newer, data_sample, trainData)
Warning messages:
1: In rm(motor_collision_crash_clean_data, data, data_newer, data_sample,  :
  object 'data_newer' not found
2: In rm(motor_collision_crash_clean_data, data, data_newer, data_sample,  :
  object 'data_sample' not found
> rm(trainData_upsampled)
> #rm(trainData_downsampled, testData)
> gc()
           used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells  2411620 128.8    7882346  421.0   7602314  406.1
Vcells 32294468 246.4  191396140 1460.3 252479278 1926.3
> 
> set.seed(123)
> ptm_rf <- proc.time()
> model_knn <- knn(
+   trainData_downsampled_without_response_var,
+   testData_without_response_var,
+   cl = trainData_downsampled_response_column,
+   k=3,
+   prob = TRUE #create probabilities so we can plot ROC
+ )
> proc.time() - ptm_rf
   user  system elapsed 
2068.56    0.30 2076.36 
> 
> #probabilities 
> attributes(model_knn)$prob
   [1] 0.6666667 0.6666667 0.7500000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667
  [14] 0.7500000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
  [27] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.5000000 0.6666667
  [40] 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
  [53] 1.0000000 1.0000000 0.6666667 0.5000000 0.6666667 0.6666667 0.7500000 1.0000000 1.0000000 0.6666667 0.6666667 0.7500000 0.6666667
  [66] 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 0.6666667
  [79] 0.5000000 0.6666667 1.0000000 0.7500000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667
  [92] 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 0.6666667 0.5000000 0.6666667
 [105] 1.0000000 0.7500000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667
 [118] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667
 [131] 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.7500000 1.0000000 0.6666667 0.7500000 0.6666667 0.5000000 0.6666667 0.6666667
 [144] 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
 [157] 1.0000000 0.6666667 0.6666667 0.7500000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000
 [170] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 1.0000000 0.6666667
 [183] 0.7500000 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000
 [196] 1.0000000 0.7500000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.7500000 0.7500000 0.6666667 1.0000000 0.6666667
 [209] 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [222] 1.0000000 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667
 [235] 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667
 [248] 0.6666667 0.5000000 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000
 [261] 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
 [274] 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667
 [287] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 0.6666667 0.7500000
 [300] 0.6666667 0.7500000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667
 [313] 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667
 [326] 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.5000000 1.0000000 0.6666667 0.5000000 0.6666667 1.0000000 0.6666667
 [339] 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 0.7500000 0.6666667 0.7500000 0.6666667 1.0000000
 [352] 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [365] 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.5000000 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [378] 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
 [391] 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667
 [404] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667
 [417] 0.5000000 0.5000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667
 [430] 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667
 [443] 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667
 [456] 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [469] 0.6666667 0.6666667 0.6666667 0.7500000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667
 [482] 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.5000000 0.6666667 0.6666667 0.7500000 0.6666667
 [495] 0.6666667 0.6666667 1.0000000 1.0000000 0.5000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000
 [508] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [521] 1.0000000 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667
 [534] 0.7500000 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000
 [547] 0.7500000 0.7500000 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
 [560] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.5000000 0.7500000 1.0000000 0.7500000 0.6666667 1.0000000 1.0000000
 [573] 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667
 [586] 0.6666667 1.0000000 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000 0.5000000 0.6666667
 [599] 1.0000000 1.0000000 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [612] 1.0000000 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000
 [625] 0.6666667 0.7500000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667
 [638] 0.5000000 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667
 [651] 1.0000000 0.6666667 0.6666667 0.7500000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.7500000
 [664] 1.0000000 0.5000000 0.6666667 0.5000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667
 [677] 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 0.6666667
 [690] 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [703] 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.7500000 1.0000000 0.6666667 0.6666667 1.0000000
 [716] 1.0000000 0.7500000 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.7500000 0.6666667
 [729] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000 0.5000000 0.7500000
 [742] 0.6666667 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 1.0000000 0.7500000
 [755] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.5000000 1.0000000 1.0000000 0.6666667 0.6666667
 [768] 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000 0.7500000 0.6666667 1.0000000 0.6666667 0.6666667
 [781] 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.7500000 0.6666667 0.6666667 0.6666667 0.6666667
 [794] 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.5000000 0.6666667 0.6666667 0.7500000 0.6666667 1.0000000
 [807] 1.0000000 0.6666667 1.0000000 0.7500000 1.0000000 1.0000000 0.7500000 0.5000000 0.7500000 1.0000000 0.6666667 0.6666667 0.6666667
 [820] 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667 1.0000000
 [833] 1.0000000 0.6666667 0.7500000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000
 [846] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000
 [859] 0.6666667 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
 [872] 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000 0.6666667 0.5000000 0.7500000 0.6666667 0.5000000
 [885] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
 [898] 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000
 [911] 0.6666667 1.0000000 0.6666667 0.5000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.7500000 0.6666667 0.6666667
 [924] 0.6666667 0.7500000 0.6666667 0.6666667 0.5000000 0.5000000 0.6666667 1.0000000 1.0000000 0.7500000 0.6666667 1.0000000 1.0000000
 [937] 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.7500000 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [950] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 1.0000000 0.7500000 0.6666667 0.6666667 1.0000000 0.5000000
 [963] 0.7500000 1.0000000 1.0000000 1.0000000 1.0000000 0.5000000 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667
 [976] 0.6666667 0.5000000 1.0000000 0.6666667 1.0000000 0.6666667 0.5000000 0.5000000 1.0000000 0.6666667 1.0000000 1.0000000 1.0000000
 [989] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 0.6666667 0.6666667 0.6666667
 [ reached getOption("max.print") -- omitted 673597 entries ]
> 
> roc.model_knn = pROC::roc(
+   testData$did_crash_happen, 
+   attributes(model_knn)$prob
+ )
Setting levels: control = no, case = yes
Setting direction: controls < cases
> auc.model_knn = pROC::auc(roc.model_knn)
> print(auc.model_knn)
Area under the curve: 0.4835
> 
> #plot ROC curve
> plot.roc(roc.model_knn, print.auc = TRUE, col = 'red' , print.thres = "best" )
> 
> confusionMatrix(model_knn, testData_response_column)
Confusion Matrix and Statistics

          Reference
Prediction     no    yes
       no  304870  61681
       yes 215703  92343
                                        
               Accuracy : 0.5888        
                 95% CI : (0.5876, 0.59)
    No Information Rate : 0.7717        
    P-Value [Acc > NIR] : 1             
                                        
                  Kappa : 0.137         
                                        
 Mcnemar's Test P-Value : <2e-16        
                                        
            Sensitivity : 0.5856        
            Specificity : 0.5995        
         Pos Pred Value : 0.8317        
         Neg Pred Value : 0.2998        
             Prevalence : 0.7717        
         Detection Rate : 0.4519        
   Detection Prevalence : 0.5434        
      Balanced Accuracy : 0.5926        
                                        
       'Positive' Class : no   